{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Machine Learning Project: Dataset Analysis and Modeling**\n",
        "## **Importing Libraries**\n",
        "\n",
        "This code cell imports the necessary libraries for data manipulation and model evaluation. It includes the pandas library for data handling, numpy for numerical operations, and scikit-learn for model selection and evaluation. The specific modules imported are pandas, numpy, train_test_split from sklearn.model_selection, and precision_score, recall_score, f1_score from sklearn.metrics. These libraries and modules will be used throughout the notebook for various tasks such as data processing, splitting the dataset, and evaluating model performance."
      ],
      "metadata": {
        "id": "l2nZN6H9BktY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "dmycNGgb8FCB"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Data Loading**\n",
        "\n",
        "This code cell reads the training and test datasets from CSV files using the pandas library. The training dataset is loaded into the 'train_data' DataFrame, and the test dataset is loaded into the 'test_data' DataFrame. The CSV files are assumed to be named 'Train_Data.csv' and 'Test_Data.csv', respectively."
      ],
      "metadata": {
        "id": "_7O65Bq_Bqcw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = pd.read_csv(\"Train_Data.csv\")\n",
        "test_data = pd.read_csv(\"Test_Data.csv\")\n"
      ],
      "metadata": {
        "id": "rMGZZoSh9WiU"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Data Preprocessing**\n",
        "\n",
        "This code cell performs several preprocessing steps on the training and test datasets."
      ],
      "metadata": {
        "id": "nSEXKPLaCTbW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking for missing values\n",
        "train_data.isnull().sum()\n",
        "\n",
        "# Handling missing values (if any)\n",
        "train_data.fillna(train_data.mean(numeric_only=True), inplace=True)\n",
        "\n",
        "# Repeating the same preprocessing steps for the test data\n",
        "test_data.fillna(test_data.mean(numeric_only=True), inplace=True)\n",
        "\n",
        "\n",
        "# Encoding categorical variables (if any)\n",
        "train_data = pd.get_dummies(train_data)\n",
        "\n",
        "# Scaling numeric features\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "train_data[['m0', 'm1', 'm2', 'm3', 'm4', 'm5', 'm6', 'm7', 'm8', 'm9', 'm10', 'm11', 'm12', 'm13', 'm14']] = scaler.fit_transform(train_data[['m0', 'm1', 'm2', 'm3', 'm4', 'm5', 'm6', 'm7', 'm8', 'm9', 'm10', 'm11', 'm12', 'm13', 'm14']])\n",
        "\n",
        "# Repeating the same preprocessing steps for the test data\n",
        "test_data = test_data.dropna()\n",
        "test_data = pd.get_dummies(test_data)\n",
        "test_data[['m0', 'm1', 'm2', 'm3', 'm4', 'm5', 'm6', 'm7', 'm8', 'm9', 'm10', 'm11', 'm12', 'm13', 'm14']] = scaler.transform(test_data[['m0', 'm1', 'm2', 'm3', 'm4', 'm5', 'm6', 'm7', 'm8', 'm9', 'm10', 'm11', 'm12', 'm13', 'm14']])\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "1adO27D1d8Se"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Train-Validation Split**\n",
        "\n",
        "This code cell performs the train-validation split on the preprocessed data to prepare it for model training and evaluation.\n",
        "The resulting X_train, X_val, y_train, and y_val datasets are now ready to be used for model training and evaluation, with X_train and y_train being the training features and target, respectively, and X_val and y_val being the validation features and target, respectively."
      ],
      "metadata": {
        "id": "bfRt11Y-C69K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "features = train_data.drop(['pred'], axis=1)  # Dropping the target column from the features\n",
        "target = train_data['pred']  # Setting the target variable\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(features, target, test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "id": "MhnDta_OefNX"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Model Initialization and Training**\n",
        "\n",
        "In this code cell, three different models are initialized and trained using the training data (X_train and y_train).\n",
        "After this code cell, the models (model1, model2, and model3) are trained and ready to make predictions on unseen data."
      ],
      "metadata": {
        "id": "sNf8XN3yDRBd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "model1 = LogisticRegression()\n",
        "model1.fit(X_train, y_train)\n",
        "\n",
        "model2 = RandomForestClassifier()\n",
        "model2.fit(X_train, y_train)\n",
        "\n",
        "model3 = SVC()\n",
        "model3.fit(X_train, y_train)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "eFurTw_49-Sg",
        "outputId": "f45de978-9f27-4514-f8da-7ff92664af85"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SVC()"
            ],
            "text/html": [
              "<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC()</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Prediction on Validation Set**\n",
        "\n",
        "This code cell applies the trained models (model1, model2, and model3) to make predictions on the validation set (X_val).\n",
        "These predictions can be used to evaluate the performance of the models and compare their results with the actual target values (y_val) for validation purposes."
      ],
      "metadata": {
        "id": "3kT1jox1D19g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pred1 = model1.predict(X_val)\n",
        "pred2 = model2.predict(X_val)\n",
        "pred3 = model3.predict(X_val)\n"
      ],
      "metadata": {
        "id": "kq7Nbyfy-SNc"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Model Evaluation**\n",
        "\n",
        "This code cell calculates precision, recall, and F1 score for each of the trained models (model1, model2, and model3) using the predictions made on the validation set (pred1, pred2, and pred3)."
      ],
      "metadata": {
        "id": "wEhS0zsNEHvG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "from sklearn.exceptions import UndefinedMetricWarning\n",
        "\n",
        "warnings.filterwarnings('ignore', category=UndefinedMetricWarning)\n",
        "\n",
        "# Calculating precision, recall, and F1 score with zero_division parameter set to 1\n",
        "with warnings.catch_warnings():\n",
        "    warnings.simplefilter('ignore')\n",
        "    precision1 = precision_score(y_val, pred1, zero_division=1)\n",
        "    recall1 = recall_score(y_val, pred1, zero_division=1)\n",
        "    f1_score1 = f1_score(y_val, pred1)\n",
        "\n",
        "    precision2 = precision_score(y_val, pred2, zero_division=1)\n",
        "    recall2 = recall_score(y_val, pred2, zero_division=1)\n",
        "    f1_score2 = f1_score(y_val, pred2)\n",
        "\n",
        "    precision3 = precision_score(y_val, pred3, zero_division=1)\n",
        "    recall3 = recall_score(y_val, pred3, zero_division=1)\n",
        "    f1_score3 = f1_score(y_val, pred3)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ltQjl5og-UA8"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Selecting the Best Model**\n",
        "\n",
        "This code cell compares the F1 scores of all the trained models (model1, model2, and model3) and selects the one with the highest F1 score as the best model.\n",
        "This allows us to identify the model that performs the best based on the F1 score, which is a commonly used metric for evaluating the overall performance of classification models."
      ],
      "metadata": {
        "id": "a1Sf0-ECEjLw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "best_model = None\n",
        "best_f1_score = 0.0\n",
        "\n",
        "# Compareing the F1 scores of all the models and select the one with the highest F1 score\n",
        "if f1_score1 > best_f1_score:\n",
        "    best_model = model1\n",
        "    best_f1_score = f1_score1\n",
        "\n",
        "if f1_score2 > best_f1_score:\n",
        "    best_model = model2\n",
        "    best_f1_score = f1_score2\n",
        "\n",
        "if f1_score3 > best_f1_score:\n",
        "    best_model = model3\n",
        "    best_f1_score = f1_score3\n"
      ],
      "metadata": {
        "id": "W9c_Mio4-2O2"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Test Data Preprocessing and Prediction**\n",
        "\n",
        "This code cell preprocesses the test data to match the preprocessing steps applied to the training data and makes predictions on the test data using the best model selected.\n",
        "These predictions represent the model's predictions on the test data, allowing us to assess the model's performance on unseen data and generate the desired outcomes."
      ],
      "metadata": {
        "id": "Tyeql8_nEv7Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "Test_Data.fillna(0, inplace=True)\n",
        "Test_Data = pd.get_dummies(Test_Data)\n",
        "\n",
        "missing_cols = set(X_train.columns) - set(Test_Data.columns)\n",
        "for col in missing_cols:\n",
        "    Test_Data[col] = 0\n",
        "Test_Data = Test_Data[X_train.columns]\n",
        "\n",
        "test_predictions = best_model.predict(Test_Data)\n"
      ],
      "metadata": {
        "id": "zwNXQTHV_Av3"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Creating Submission File**\n",
        "\n",
        "This code cell creates a submission file containing the predictions made on the test data using the best model."
      ],
      "metadata": {
        "id": "K0uyZ2mRE8sV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "submission = pd.DataFrame({'pred': test_predictions})\n",
        "submission.to_csv(\"predictions.csv\", index=False)\n"
      ],
      "metadata": {
        "id": "xcCAXh8V_bUA"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Printing Best Model F1 Score**\n",
        "\n",
        "This code cell prints the F1 score of the best model selected based on the comparison of F1 scores from the previous code cells."
      ],
      "metadata": {
        "id": "GbEwGjsZFK_D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Best Model F1 Score:\", best_f1_score)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eGs9YWE4_teP",
        "outputId": "3471fc1f-1762-4172-df93-779dde93ddc7"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Model F1 Score: 0.008307372793354102\n"
          ]
        }
      ]
    }
  ]
}